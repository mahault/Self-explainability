{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBmrueUn2ocmQ74q0x7UwI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahault/Self-explainability/blob/main/explainability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJxWRPWrycWR",
        "outputId": "6d2f9146-a120-464a-9779-f2c073b8b16c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The AI chose its action primarily based on state 2, which had the highest influence score (0.049). The AI is confident in its self-explanation.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class ExplainableActiveInference:\n",
        "    \"\"\"\n",
        "    Implements an explainable AI agent using Active Inference principles.\n",
        "    This model replaces static influence scoring with a hierarchical generative model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, state_size):\n",
        "        # Initialize priors and uncertainties\n",
        "        self.state_size = state_size\n",
        "        self.prior_influence = np.zeros(state_size)  # Influence scores for each state\n",
        "        self.belief_uncertainty = np.ones(state_size)  # Higher means more uncertainty\n",
        "\n",
        "    def infer_influence(self, selected_action, qs, B):\n",
        "        \"\"\"\n",
        "        Infers influence of past states on action selection using Bayesian updates.\n",
        "        Returns:\n",
        "            influence_scores: Bayesian-updated influence estimates\n",
        "            uncertainty_scores: Self-tracked uncertainty of influence\n",
        "        \"\"\"\n",
        "        ACTIONABLE_ROW = 0\n",
        "        action_taken = selected_action[ACTIONABLE_ROW]\n",
        "        latest_beliefs = qs[ACTIONABLE_ROW]  # Belief distribution over states\n",
        "\n",
        "        influence_scores = np.zeros(self.state_size)\n",
        "        uncertainty_scores = np.zeros(self.state_size)\n",
        "\n",
        "        for state_idx in range(self.state_size):\n",
        "            belief = latest_beliefs[state_idx]  # Probability of being in this state\n",
        "\n",
        "            # Probabilistic transition to next state\n",
        "            if hasattr(B, '__getitem__'):\n",
        "                next_state_prob = B[ACTIONABLE_ROW][state_idx]\n",
        "            else:\n",
        "                next_state_prob = np.eye(self.state_size)[state_idx]  # Identity transition if B is missing\n",
        "\n",
        "            # Expected free energy contribution (Bayesian approach)\n",
        "            influence_score = belief * np.mean(next_state_prob)\n",
        "\n",
        "            # Bayesian Update of Prior Belief\n",
        "            self.prior_influence[state_idx] = (self.prior_influence[state_idx] + influence_score) / 2\n",
        "\n",
        "            # Update uncertainty: tracking variance of influence scores\n",
        "            uncertainty_scores[state_idx] = np.std([self.prior_influence[state_idx], influence_score])\n",
        "\n",
        "        self.belief_uncertainty = uncertainty_scores\n",
        "        return self.prior_influence, uncertainty_scores\n",
        "\n",
        "    def introspection_score(self):\n",
        "        \"\"\"\n",
        "        Measures how well the AI understands its own decision influences.\n",
        "        Returns:\n",
        "            A score between 0 and 1 (higher = more self-aware)\n",
        "        \"\"\"\n",
        "        mean_uncertainty = np.mean(self.belief_uncertainty)\n",
        "        return 1 / (1 + mean_uncertainty)  # Higher score means lower uncertainty (better explainability)\n",
        "\n",
        "    def explain_decision(self):\n",
        "        \"\"\"\n",
        "        Generates a human-readable explanation of why the AI made its decision.\n",
        "        Returns:\n",
        "            A textual explanation based on influence and introspection scores.\n",
        "        \"\"\"\n",
        "        max_influence_state = np.argmax(self.prior_influence)\n",
        "        introspection_quality = self.introspection_score()\n",
        "\n",
        "        explanation = (\n",
        "            f\"The AI chose its action primarily based on state {max_influence_state}, \"\n",
        "            f\"which had the highest influence score ({self.prior_influence[max_influence_state]:.3f}). \"\n",
        "        )\n",
        "\n",
        "        if introspection_quality > 0.8:\n",
        "            explanation += \"The AI is confident in its self-explanation.\"\n",
        "        elif introspection_quality > 0.5:\n",
        "            explanation += \"The AI has moderate confidence in its explanation.\"\n",
        "        else:\n",
        "            explanation += \"The AI has high uncertainty about why it made this decision.\"\n",
        "\n",
        "        return explanation\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    state_size = 10  # Assume 10 possible states\n",
        "    ai_system = ExplainableActiveInference(state_size)\n",
        "\n",
        "    # Example action (4 possible actions)\n",
        "    selected_action = np.array([0.1, 0.2, 0.5, 0.2])\n",
        "\n",
        "    # Example belief state distribution\n",
        "    qs = np.array([[0.05, 0.1, 0.3, 0.1, 0.15, 0.1, 0.05, 0.05, 0.05, 0.05]])\n",
        "\n",
        "    # Example transition model (B-matrix)\n",
        "    B = np.random.rand(1, state_size, state_size)  # Random state transitions\n",
        "\n",
        "    # Perform Bayesian influence inference\n",
        "    influence_scores, uncertainty_scores = ai_system.infer_influence(selected_action, qs, B)\n",
        "\n",
        "    # Print explanation\n",
        "    print(ai_system.explain_decision())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class HierarchicalGenerativeModelAI:\n",
        "    \"\"\"\n",
        "    Implements a hierarchical generative model for explainable AI.\n",
        "    This model has three levels:\n",
        "    1. Overt action layer: standard Active Inference.\n",
        "    2. Covert policy selection layer: tracks influence of beliefs on actions.\n",
        "    3. Meta-introspection layer: evaluates self-awareness of decision-making.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # **Level 1: Beliefs about states**\n",
        "        self.qs = np.ones(state_size) / state_size  # Belief over states\n",
        "        self.B = np.random.rand(state_size, state_size)  # Transition matrix\n",
        "        self.A = np.random.rand(state_size, state_size)  # Likelihood matrix P(o|s)\n",
        "\n",
        "        # **Level 2: Policy selection**\n",
        "        self.qpi = np.ones(action_size) / action_size  # Belief over policies\n",
        "        self.G = np.random.rand(action_size)  # Expected Free Energy (EFE)\n",
        "\n",
        "        # **Level 3: Introspection layer**\n",
        "        self.U = np.ones(state_size) / state_size  # Belief about self-awareness\n",
        "        self.gamma = np.random.rand(state_size)  # Attentional weight for introspection\n",
        "\n",
        "    def infer_states(self, observation):\n",
        "        \"\"\"\n",
        "        Level 1: Bayesian state inference P(s_t | o_t).\n",
        "        \"\"\"\n",
        "        likelihood = self.A[:, observation]  # P(o_t | s_t)\n",
        "        self.qs = likelihood * self.qs  # Bayesian update\n",
        "        self.qs /= np.sum(self.qs)  # Normalize\n",
        "\n",
        "    def infer_policies(self):\n",
        "        \"\"\"\n",
        "        Level 2: Infer best policy using Expected Free Energy (EFE).\n",
        "        \"\"\"\n",
        "        expected_outcomes = self.B @ self.qs  # Predicted future states\n",
        "\n",
        "        # Ensure probabilities sum to 1 (avoid division by zero)\n",
        "        expected_outcomes = expected_outcomes / (np.sum(expected_outcomes) + 1e-6)\n",
        "\n",
        "        # Compute Expected Free Energy (EFE) per action (assume B maps to actions)\n",
        "        self.G = -np.sum(expected_outcomes * np.log(expected_outcomes + 1e-6))  # Still a scalar\n",
        "\n",
        "        # Ensure `G` is a vector over `action_size`\n",
        "        self.G = np.repeat(self.G, self.action_size)  # Expand into a vector\n",
        "\n",
        "        # Convert EFE to policy probabilities\n",
        "        self.qpi = np.exp(-self.G)  # Convert EFE scores to probabilities\n",
        "        self.qpi /= np.sum(self.qpi)  # Normalize\n",
        "\n",
        "        # Validate that qpi is properly normalized\n",
        "        if not np.isclose(np.sum(self.qpi), 1.0, atol=1e-6):\n",
        "            raise ValueError(f\"qpi normalization failed, sum={np.sum(self.qpi)}\")\n",
        "\n",
        "        # Ensure `qpi` has the correct shape\n",
        "        if self.qpi.shape[0] != self.action_size:\n",
        "            raise ValueError(f\"qpi must be a probability vector of size {self.action_size}, got shape {self.qpi.shape}\")\n",
        "\n",
        "\n",
        "    def infer_introspection(self):\n",
        "        \"\"\"\n",
        "        Level 3: Meta-awareness of decision-making confidence.\n",
        "        \"\"\"\n",
        "        self.U = 1 / (1 + np.std(self.qs))  # Self-awareness is inverse of uncertainty\n",
        "        self.gamma = np.exp(-self.U)  # Attentional modulation based on introspection\n",
        "\n",
        "    def select_action(self):\n",
        "        \"\"\"\n",
        "        Sample an action from the inferred policy.\n",
        "        \"\"\"\n",
        "        if len(self.qpi.shape) == 0 or len(self.qpi) != self.action_size:\n",
        "            raise ValueError(f\"qpi must be a probability vector of size {self.action_size}, got {self.qpi.shape}\")\n",
        "\n",
        "        action = np.random.choice(self.action_size, p=self.qpi)\n",
        "        return action\n",
        "\n",
        "\n",
        "    def explain_decision(self):\n",
        "        \"\"\"\n",
        "        Generates a hierarchical explanation of the AI's decision.\n",
        "        \"\"\"\n",
        "        chosen_state = np.argmax(self.qs)\n",
        "        introspection_quality = np.mean(self.U)\n",
        "\n",
        "        explanation = (\n",
        "            f\"Level 1: The AI believes it is in state {chosen_state} \"\n",
        "            f\"with probability {self.qs[chosen_state]:.3f}.\\n\"\n",
        "            f\"Level 2: The selected policy has an expected free energy of {self.G.min():.3f}.\\n\"\n",
        "            f\"Level 3: The AI's introspection confidence is {introspection_quality:.3f}.\"\n",
        "        )\n",
        "\n",
        "        if introspection_quality > 0.8:\n",
        "            explanation += \" The AI is highly confident in its explanation.\"\n",
        "        elif introspection_quality > 0.5:\n",
        "            explanation += \" The AI has moderate confidence in its explanation.\"\n",
        "        else:\n",
        "            explanation += \" The AI has high uncertainty about its reasoning.\"\n",
        "\n",
        "        return explanation\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    state_size = 5  # Assume 5 possible states\n",
        "    action_size = 3  # Assume 3 possible actions\n",
        "\n",
        "    ai_system = HierarchicalGenerativeModelAI(state_size, action_size)\n",
        "\n",
        "    # Example observation (randomly chosen)\n",
        "    observation = np.random.randint(0, state_size)\n",
        "\n",
        "    # Perform Bayesian state inference\n",
        "    ai_system.infer_states(observation)\n",
        "\n",
        "    # Infer the best policy\n",
        "    ai_system.infer_policies()\n",
        "\n",
        "    # Perform meta-introspection\n",
        "    ai_system.infer_introspection()\n",
        "\n",
        "    # Select an action\n",
        "    action = ai_system.select_action()\n",
        "\n",
        "    # Explain the decision\n",
        "    print(ai_system.explain_decision())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZFYZBkxydt1",
        "outputId": "cd0e3bf9-9c80-4014-a5bf-e72221c41eed"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Level 1: The AI believes it is in state 2 with probability 0.246.\n",
            "Level 2: The selected policy has an expected free energy of 1.542.\n",
            "Level 3: The AI's introspection confidence is 0.943. The AI is highly confident in its explanation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class ExplainableActiveInference:\n",
        "    \"\"\"\n",
        "    Implements an explainable AI agent using Active Inference principles.\n",
        "    Now explicitly tracks belief updates and action selection reasoning.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # **Level 1: Beliefs about states**\n",
        "        self.qs = np.ones(state_size) / state_size  # Belief over states\n",
        "        self.B = np.random.rand(state_size, state_size)  # Transition matrix\n",
        "        self.A = np.random.rand(state_size, state_size)  # Likelihood matrix P(o|s)\n",
        "\n",
        "        # **Level 2: Policy selection**\n",
        "        self.qpi = np.ones(action_size) / action_size  # Belief over policies\n",
        "        self.G = np.zeros(action_size)  # Expected Free Energy (EFE)\n",
        "\n",
        "        # **Level 3: Introspection layer**\n",
        "        self.U = np.ones(state_size) / state_size  # Belief about self-awareness\n",
        "        self.gamma = np.random.rand(state_size)  # Attentional weight for introspection\n",
        "\n",
        "        # Storage for explainability\n",
        "        self.belief_trace = []\n",
        "        self.action_trace = []\n",
        "\n",
        "    def infer_states(self, observation):\n",
        "        \"\"\"\n",
        "        Level 1: Bayesian state inference P(s_t | o_t).\n",
        "        Now tracks which observations contributed most to belief updates.\n",
        "        \"\"\"\n",
        "        likelihood = self.A[:, observation]  # P(o_t | s_t)\n",
        "        prior_belief = self.qs.copy()\n",
        "\n",
        "        # Bayesian update\n",
        "        self.qs = likelihood * self.qs\n",
        "        self.qs /= np.sum(self.qs)  # Normalize\n",
        "\n",
        "        # Identify which state changed the most\n",
        "        belief_changes = np.abs(self.qs - prior_belief)\n",
        "        most_updated_state = np.argmax(belief_changes)\n",
        "\n",
        "        # Store trace of belief update\n",
        "        self.belief_trace.append({\n",
        "            \"prior_belief\": prior_belief,\n",
        "            \"updated_belief\": self.qs,\n",
        "            \"most_updated_state\": most_updated_state,\n",
        "            \"likelihood_used\": likelihood\n",
        "        })\n",
        "\n",
        "    def infer_policies(self):\n",
        "        \"\"\"\n",
        "        Level 2: Infer best policy using Expected Free Energy (EFE).\n",
        "        Now explicitly tracks rejected alternative actions.\n",
        "        \"\"\"\n",
        "        expected_outcomes = self.B @ self.qs  # Predicted future states\n",
        "\n",
        "        # Compute Expected Free Energy (EFE) per action\n",
        "        for action in range(self.action_size):\n",
        "            # Simulate taking an action by considering expected transition\n",
        "            future_state = expected_outcomes[action]\n",
        "            self.G[action] = -np.sum(future_state * np.log(future_state + 1e-6))  # EFE\n",
        "\n",
        "        # Convert EFE to policy probabilities\n",
        "        self.qpi = np.exp(-self.G)\n",
        "        self.qpi /= np.sum(self.qpi)\n",
        "\n",
        "        # Store action reasoning trace\n",
        "        sorted_actions = np.argsort(self.G)  # Rank actions by EFE (lower is better)\n",
        "        self.action_trace.append({\n",
        "            \"EFE_values\": self.G,\n",
        "            \"best_action\": sorted_actions[0],\n",
        "            \"rejected_actions\": sorted_actions[1:].tolist()\n",
        "        })\n",
        "\n",
        "    def select_action(self):\n",
        "        \"\"\"\n",
        "        Selects an action based on the inferred policy.\n",
        "        \"\"\"\n",
        "        action = np.random.choice(self.action_size, p=self.qpi)\n",
        "        return action\n",
        "\n",
        "    def explain_decision(self):\n",
        "        \"\"\"\n",
        "        Generates a structured explanation including belief updates and action selection.\n",
        "        \"\"\"\n",
        "        chosen_state = np.argmax(self.qs)\n",
        "        introspection_quality = np.mean(self.U)\n",
        "\n",
        "        # Get belief update reasoning\n",
        "        belief_info = self.belief_trace[-1]\n",
        "        action_info = self.action_trace[-1]\n",
        "\n",
        "        explanation = (\n",
        "            f\"Level 1: The AI believes it is in state {chosen_state} \"\n",
        "            f\"with probability {self.qs[chosen_state]:.3f}, based on observation likelihoods {belief_info['likelihood_used']}.\\n\"\n",
        "            f\"Most updated state: {belief_info['most_updated_state']}.\\n\"\n",
        "            f\"Level 2: The selected policy has an expected free energy of {self.G.min():.3f}.\\n\"\n",
        "            f\"Action Selection: The AI chose action {action_info['best_action']} \"\n",
        "            f\"because it had the lowest EFE ({self.G[action_info['best_action']]:.3f}).\\n\"\n",
        "            f\"Rejected actions: {action_info['rejected_actions']} \"\n",
        "            f\"with higher EFEs: {[self.G[a] for a in action_info['rejected_actions']]}.\\n\"\n",
        "            f\"Level 3: The AI's introspection confidence is {introspection_quality:.3f}.\"\n",
        "        )\n",
        "\n",
        "        return explanation\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    state_size = 5\n",
        "    action_size = 3\n",
        "\n",
        "    ai_system = ExplainableActiveInference(state_size, action_size)\n",
        "\n",
        "    # Example observation\n",
        "    observation = np.random.randint(0, state_size)\n",
        "\n",
        "    # Perform Bayesian state inference\n",
        "    ai_system.infer_states(observation)\n",
        "\n",
        "    # Infer the best policy\n",
        "    ai_system.infer_policies()\n",
        "\n",
        "    # Select an action\n",
        "    action = ai_system.select_action()\n",
        "\n",
        "    # Explain the decision\n",
        "    print(ai_system.explain_decision())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYEWzRIk0jZ0",
        "outputId": "649cab58-44da-4603-fbdd-7e6c602fd76a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Level 1: The AI believes it is in state 2 with probability 0.656, based on observation likelihoods [0.10598477 0.28071784 0.90216317 0.0496934  0.03588424].\n",
            "Most updated state: 2.\n",
            "Level 2: The selected policy has an expected free energy of 0.306.\n",
            "Action Selection: The AI chose action 1 because it had the lowest EFE (0.306).\n",
            "Rejected actions: [0, 2] with higher EFEs: [0.358424395146506, 0.367870128218928].\n",
            "Level 3: The AI's introspection confidence is 0.200.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class ExplainableGenerativeAI:\n",
        "    \"\"\"\n",
        "    Implements a full generative model for explainability.\n",
        "    Now explicitly tracks:\n",
        "    1. Likelihood updates (P(o|s))\n",
        "    2. Variational Free Energy (VFE)\n",
        "    3. Expected Free Energy (EFE) components (epistemic + pragmatic)\n",
        "    4. How uncertainty propagates to introspection confidence\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # **Level 1: State beliefs**\n",
        "        self.qs = np.ones(state_size) / state_size  # Belief over states\n",
        "        self.B = np.random.rand(state_size, state_size)  # Transition matrix\n",
        "        self.A = np.random.rand(state_size, state_size)  # Likelihood matrix P(o|s)\n",
        "\n",
        "        # **Level 2: Policy selection**\n",
        "        self.qpi = np.ones(action_size) / action_size  # Policy selection distribution\n",
        "        self.G = np.zeros(action_size)  # Expected Free Energy (EFE)\n",
        "\n",
        "        # **Level 3: Introspection layer**\n",
        "        self.U = np.ones(state_size) / state_size  # Belief about self-awareness\n",
        "        self.gamma = np.random.rand(state_size)  # Attentional weight for introspection\n",
        "\n",
        "        # Tracking for explainability\n",
        "        self.belief_trace = []\n",
        "        self.action_trace = []\n",
        "        self.vfe_trace = []\n",
        "\n",
        "    def infer_states(self, observation):\n",
        "        \"\"\"\n",
        "        Level 1: Bayesian inference over states.\n",
        "        Now explicitly tracks likelihood updates and Variational Free Energy (VFE).\n",
        "        \"\"\"\n",
        "        prior_belief = self.qs.copy()\n",
        "        likelihood = self.A[:, observation]  # P(o_t | s_t)\n",
        "\n",
        "        # Compute Variational Free Energy (VFE)\n",
        "        vfe = -np.sum(self.qs * np.log(likelihood + 1e-6))\n",
        "\n",
        "        # Bayesian update\n",
        "        self.qs = likelihood * self.qs\n",
        "        self.qs /= np.sum(self.qs)  # Normalize\n",
        "\n",
        "        # Identify which state changed the most\n",
        "        belief_changes = np.abs(self.qs - prior_belief)\n",
        "        most_updated_state = np.argmax(belief_changes)\n",
        "\n",
        "        # Store traces for explanation\n",
        "        self.belief_trace.append({\n",
        "            \"prior_belief\": prior_belief,\n",
        "            \"updated_belief\": self.qs,\n",
        "            \"most_updated_state\": most_updated_state,\n",
        "            \"likelihood_used\": likelihood,\n",
        "            \"VFE\": vfe\n",
        "        })\n",
        "        self.vfe_trace.append(vfe)\n",
        "\n",
        "    def infer_policies(self):\n",
        "        \"\"\"\n",
        "        Level 2: Infer best policy using Expected Free Energy (EFE).\n",
        "        Now explicitly tracks epistemic and pragmatic contributions.\n",
        "        \"\"\"\n",
        "        # Generate expected outcomes over actions, not states\n",
        "        expected_outcomes = np.random.rand(self.action_size, self.state_size)  # Placeholder for action-state transitions\n",
        "\n",
        "        # Ensure expected_outcomes is correctly shaped\n",
        "        if expected_outcomes.ndim == 1:\n",
        "            expected_outcomes = expected_outcomes.reshape(-1, 1)\n",
        "\n",
        "        # Compute separate components of EFE for each action\n",
        "        epistemic_value = -np.sum(expected_outcomes * np.log(expected_outcomes + 1e-6), axis=1)  # Over actions\n",
        "        pragmatic_value = -np.sum(expected_outcomes * np.log(self.qs + 1e-6), axis=1)  # Over actions\n",
        "\n",
        "        # Ensure both components are vectors of shape (action_size,)\n",
        "        self.G = epistemic_value + pragmatic_value  # Full EFE\n",
        "\n",
        "        # Convert EFE to policy probabilities\n",
        "        self.qpi = np.exp(-self.G)\n",
        "        self.qpi /= np.sum(self.qpi)\n",
        "\n",
        "        # Ensure `qpi` is a vector of correct size\n",
        "        if self.qpi.shape != (self.action_size,):\n",
        "            raise ValueError(f\"qpi must be a probability vector of size {self.action_size}, but got shape {self.qpi.shape}\")\n",
        "\n",
        "        # Store action reasoning trace\n",
        "        sorted_actions = np.argsort(self.G)  # Rank actions by EFE (lower is better)\n",
        "        self.action_trace.append({\n",
        "            \"EFE_values\": self.G,\n",
        "            \"best_action\": sorted_actions[0],\n",
        "            \"rejected_actions\": sorted_actions[1:].tolist(),\n",
        "            \"epistemic_value\": epistemic_value,\n",
        "            \"pragmatic_value\": pragmatic_value\n",
        "        })\n",
        "    def select_action(self):\n",
        "        \"\"\"\n",
        "        Selects an action based on the inferred policy.\n",
        "        \"\"\"\n",
        "        action = np.random.choice(self.action_size, p=self.qpi)\n",
        "        return action\n",
        "\n",
        "    def explain_decision(self):\n",
        "        \"\"\"\n",
        "        Generates a structured explanation including:\n",
        "        - Variational Free Energy (VFE) for belief updates\n",
        "        - Epistemic vs. pragmatic components of Expected Free Energy (EFE)\n",
        "        - A causal breakdown of how decisions were made\n",
        "        \"\"\"\n",
        "        chosen_state = np.argmax(self.qs)\n",
        "        introspection_quality = np.mean(self.U)\n",
        "\n",
        "        # Get belief update reasoning\n",
        "        belief_info = self.belief_trace[-1]\n",
        "        action_info = self.action_trace[-1]\n",
        "        vfe = belief_info[\"VFE\"]\n",
        "\n",
        "        # Find key contributing factors to belief update\n",
        "        prior_belief = belief_info[\"prior_belief\"]\n",
        "        updated_belief = belief_info[\"updated_belief\"]\n",
        "        belief_shift = updated_belief - prior_belief\n",
        "        belief_shift_state = np.argmax(np.abs(belief_shift))\n",
        "\n",
        "        # Find key contributing factors to EFE\n",
        "        best_action = action_info[\"best_action\"]\n",
        "        rejected_actions = action_info[\"rejected_actions\"]\n",
        "        epistemic_value = action_info[\"epistemic_value\"]\n",
        "        pragmatic_value = action_info[\"pragmatic_value\"]\n",
        "\n",
        "        # Explanation for belief update\n",
        "        belief_explanation = (\n",
        "            f\"Belief Update:\\n\"\n",
        "            f\"- The AI originally believed it was most likely in state {np.argmax(prior_belief)}, but observation \"\n",
        "            f\"caused belief to shift towards state {chosen_state}.\\n\"\n",
        "            f\"- The state that changed the most was {belief_shift_state}, with a shift of {belief_shift[belief_shift_state]:.3f}.\\n\"\n",
        "            f\"- This change was driven by observation likelihoods {belief_info['likelihood_used']}.\\n\"\n",
        "            f\"- Variational Free Energy (VFE) was {vfe:.3f}, meaning uncertainty decreased by this amount.\"\n",
        "        )\n",
        "\n",
        "        # Explanation for action selection\n",
        "        efe_explanation = (\n",
        "            f\"Action Selection:\\n\"\n",
        "            f\"- The AI chose action {best_action} because it had the lowest Expected Free Energy (EFE): {self.G[best_action]:.3f}.\\n\"\n",
        "            f\"- Epistemic Value (expected uncertainty reduction): {epistemic_value[best_action]:.3f}.\\n\"\n",
        "            f\"- Pragmatic Value (goal alignment): {pragmatic_value[best_action]:.3f}.\\n\"\n",
        "            f\"- Rejected actions: {rejected_actions} with higher EFEs: {[self.G[a] for a in rejected_actions]}.\\n\"\n",
        "            f\"- The AI rejected action {rejected_actions[0]} because it had a lower epistemic value ({epistemic_value[rejected_actions[0]]:.3f}) \"\n",
        "            f\"and a less favorable goal alignment ({pragmatic_value[rejected_actions[0]]:.3f}).\"\n",
        "        )\n",
        "\n",
        "        # Explanation for confidence\n",
        "        introspection_explanation = (\n",
        "            f\"Introspection:\\n\"\n",
        "            f\"- The AI's introspection confidence is {introspection_quality:.3f}.\\n\"\n",
        "            f\"- Confidence is lower because the belief update had high variance, suggesting multiple competing explanations for the observations.\"\n",
        "        )\n",
        "\n",
        "        return f\"{belief_explanation}\\n\\n{efe_explanation}\\n\\n{introspection_explanation}\"\n",
        "\n",
        "# Running the model\n",
        "state_size = 5\n",
        "action_size = 3\n",
        "\n",
        "ai_system = ExplainableGenerativeAI(state_size, action_size)\n",
        "\n",
        "# Example observation\n",
        "observation = np.random.randint(0, state_size)\n",
        "\n",
        "# Perform Bayesian state inference\n",
        "ai_system.infer_states(observation)\n",
        "\n",
        "# Infer the best policy\n",
        "ai_system.infer_policies()\n",
        "\n",
        "# Select an action\n",
        "action = ai_system.select_action()\n",
        "\n",
        "# Explain the decision\n",
        "explanation = ai_system.explain_decision()\n",
        "explanation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "plg7RD_x4ep-",
        "outputId": "b642c377-6645-49a7-f459-79df884679a2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Belief Update:\\n- The AI originally believed it was most likely in state 0, but observation caused belief to shift towards state 0.\\n- The state that changed the most was 1, with a shift of -0.197.\\n- This change was driven by observation likelihoods [0.94458641 0.00747353 0.86784213 0.69144075 0.17349219].\\n- Variational Free Energy (VFE) was 1.443, meaning uncertainty decreased by this amount.\\n\\nAction Selection:\\n- The AI chose action 2 because it had the lowest Expected Free Energy (EFE): 6.309.\\n- Epistemic Value (expected uncertainty reduction): 0.919.\\n- Pragmatic Value (goal alignment): 5.390.\\n- Rejected actions: [0, 1] with higher EFEs: [7.173054832999318, 9.343246138265695].\\n- The AI rejected action 0 because it had a lower epistemic value (1.705) and a less favorable goal alignment (5.468).\\n\\nIntrospection:\\n- The AI's introspection confidence is 0.200.\\n- Confidence is lower because the belief update had high variance, suggesting multiple competing explanations for the observations.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class ExplainableGenerativeAI:\n",
        "    \"\"\"\n",
        "    Implements a full generative model for explainability.\n",
        "    Now explicitly tracks:\n",
        "    1. Likelihood updates (P(o|s))\n",
        "    2. Variational Free Energy (VFE)\n",
        "    3. Expected Free Energy (EFE) components (epistemic + pragmatic)\n",
        "    4. How uncertainty propagates to introspection confidence\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # **Level 1: State beliefs**\n",
        "        self.qs = np.ones(state_size) / state_size  # Belief over states\n",
        "        self.B = np.random.rand(state_size, state_size)  # Transition matrix\n",
        "        self.A = np.random.rand(state_size, state_size)  # Likelihood matrix P(o|s)\n",
        "\n",
        "        # **Level 2: Policy selection**\n",
        "        self.qpi = np.ones(action_size) / action_size  # Policy selection distribution\n",
        "        self.G = np.zeros(action_size)  # Expected Free Energy (EFE)\n",
        "\n",
        "        # **Level 3: Introspection layer**\n",
        "        self.U = np.ones(state_size) / state_size  # Belief about self-awareness\n",
        "        self.gamma = np.random.rand(state_size)  # Attentional weight for introspection\n",
        "\n",
        "        # Tracking for explainability\n",
        "        self.belief_trace = []\n",
        "        self.action_trace = []\n",
        "        self.vfe_trace = []\n",
        "\n",
        "    def infer_states(self, observation):\n",
        "        \"\"\"\n",
        "        Level 1: Bayesian inference over states.\n",
        "        Now explicitly tracks likelihood updates and Variational Free Energy (VFE).\n",
        "        \"\"\"\n",
        "        prior_belief = self.qs.copy()\n",
        "        likelihood = self.A[:, observation]  # P(o_t | s_t)\n",
        "\n",
        "        # Compute Variational Free Energy (VFE)\n",
        "        vfe = -np.sum(self.qs * np.log(likelihood + 1e-6))\n",
        "\n",
        "        # Bayesian update\n",
        "        self.qs = likelihood * self.qs\n",
        "        self.qs /= np.sum(self.qs)  # Normalize\n",
        "\n",
        "        # Identify which state changed the most\n",
        "        belief_changes = np.abs(self.qs - prior_belief)\n",
        "        most_updated_state = np.argmax(belief_changes)\n",
        "\n",
        "        # Store traces for explanation\n",
        "        self.belief_trace.append({\n",
        "            \"prior_belief\": prior_belief,\n",
        "            \"updated_belief\": self.qs,\n",
        "            \"most_updated_state\": most_updated_state,\n",
        "            \"likelihood_used\": likelihood,\n",
        "            \"VFE\": vfe\n",
        "        })\n",
        "        self.vfe_trace.append(vfe)\n",
        "\n",
        "    def infer_policies(self):\n",
        "        \"\"\"\n",
        "        Level 2: Infer best policy using Expected Free Energy (EFE).\n",
        "        Now explicitly tracks epistemic and pragmatic contributions.\n",
        "        \"\"\"\n",
        "        # Generate expected outcomes over actions, not states\n",
        "        expected_outcomes = np.random.rand(self.action_size, self.state_size)  # Placeholder for action-state transitions\n",
        "\n",
        "        # Ensure expected_outcomes is correctly shaped\n",
        "        if expected_outcomes.ndim == 1:\n",
        "            expected_outcomes = expected_outcomes.reshape(-1, 1)\n",
        "\n",
        "        # Compute separate components of EFE for each action\n",
        "        epistemic_value = -np.sum(expected_outcomes * np.log(expected_outcomes + 1e-6), axis=1)  # Over actions\n",
        "        pragmatic_value = -np.sum(expected_outcomes * np.log(self.qs + 1e-6), axis=1)  # Over actions\n",
        "\n",
        "        # Ensure both components are vectors of shape (action_size,)\n",
        "        self.G = epistemic_value + pragmatic_value  # Full EFE\n",
        "\n",
        "        # Convert EFE to policy probabilities\n",
        "        self.qpi = np.exp(-self.G)\n",
        "        self.qpi /= np.sum(self.qpi)\n",
        "\n",
        "        # Ensure `qpi` is a vector of correct size\n",
        "        if self.qpi.shape != (self.action_size,):\n",
        "            raise ValueError(f\"qpi must be a probability vector of size {self.action_size}, but got shape {self.qpi.shape}\")\n",
        "\n",
        "        # Store action reasoning trace\n",
        "        sorted_actions = np.argsort(self.G)  # Rank actions by EFE (lower is better)\n",
        "        self.action_trace.append({\n",
        "            \"EFE_values\": self.G,\n",
        "            \"best_action\": sorted_actions[0],\n",
        "            \"rejected_actions\": sorted_actions[1:].tolist(),\n",
        "            \"epistemic_value\": epistemic_value,\n",
        "            \"pragmatic_value\": pragmatic_value\n",
        "        })\n",
        "    def select_action(self):\n",
        "        \"\"\"\n",
        "        Selects an action based on the inferred policy.\n",
        "        \"\"\"\n",
        "        action = np.random.choice(self.action_size, p=self.qpi)\n",
        "        return action\n",
        "\n",
        "    def explain_decision(self):\n",
        "        \"\"\"\n",
        "        Generates a structured explanation including:\n",
        "        - Variational Free Energy (VFE) for belief updates\n",
        "        - Epistemic vs. pragmatic components of Expected Free Energy (EFE)\n",
        "        - A causal breakdown of how decisions were made\n",
        "        \"\"\"\n",
        "        chosen_state = np.argmax(self.qs)  # Most likely current state\n",
        "        prior_state = np.argmax(self.belief_trace[-1][\"prior_belief\"])  # Most likely state before update\n",
        "        introspection_quality = np.mean(self.U)\n",
        "\n",
        "        # Get belief update reasoning\n",
        "        belief_info = self.belief_trace[-1]\n",
        "        action_info = self.action_trace[-1]\n",
        "        vfe = belief_info[\"VFE\"]\n",
        "\n",
        "        # Find key contributing factors to belief update\n",
        "        prior_belief = belief_info[\"prior_belief\"]\n",
        "        updated_belief = belief_info[\"updated_belief\"]\n",
        "        belief_shift = updated_belief - prior_belief\n",
        "        belief_shift_state = np.argmax(np.abs(belief_shift))\n",
        "\n",
        "        # Identify the likelihood that caused the biggest shift\n",
        "        dominant_observation = np.argmax(belief_info[\"likelihood_used\"])\n",
        "\n",
        "        # Fix the belief update explanation\n",
        "        if chosen_state == prior_state:\n",
        "            belief_update_text = (\n",
        "                f\"The AI originally believed it was in state {prior_state}, and its belief remained the same after processing the observation. \"\n",
        "                f\"However, the belief in state {belief_shift_state} changed significantly by {belief_shift[belief_shift_state]:.3f}, \"\n",
        "                f\"due to the likelihood of observation {dominant_observation} being high ({belief_info['likelihood_used'][dominant_observation]:.3f}).\"\n",
        "            )\n",
        "        else:\n",
        "            belief_update_text = (\n",
        "                f\"The AI originally believed it was in state {prior_state}, but observation led it to shift its belief towards state {chosen_state}. \"\n",
        "                f\"The biggest belief shift occurred in state {belief_shift_state}, changing by {belief_shift[belief_shift_state]:.3f}, \"\n",
        "                f\"driven by observation {dominant_observation} with likelihood {belief_info['likelihood_used'][dominant_observation]:.3f}.\"\n",
        "            )\n",
        "\n",
        "        # Explanation for action selection\n",
        "        best_action = action_info[\"best_action\"]\n",
        "        rejected_actions = action_info[\"rejected_actions\"]\n",
        "        epistemic_value = action_info[\"epistemic_value\"]\n",
        "        pragmatic_value = action_info[\"pragmatic_value\"]\n",
        "\n",
        "        efe_explanation = (\n",
        "            f\"Action Selection:\\n\"\n",
        "            f\"- The AI chose action {best_action} because it had the lowest Expected Free Energy (EFE): {self.G[best_action]:.3f}.\\n\"\n",
        "            f\"- This means action {best_action} is expected to provide the best trade-off between gaining useful information and achieving goals.\\n\"\n",
        "            f\"- Epistemic Value (expected uncertainty reduction): {epistemic_value[best_action]:.3f}.\\n\"\n",
        "            f\"- Pragmatic Value (goal alignment): {pragmatic_value[best_action]:.3f}.\\n\"\n",
        "            f\"- The AI rejected action {rejected_actions[0]} because it had a lower epistemic value ({epistemic_value[rejected_actions[0]]:.3f}) \"\n",
        "            f\"and a less favorable goal alignment ({pragmatic_value[rejected_actions[0]]:.3f}).\"\n",
        "        )\n",
        "\n",
        "        introspection_explanation = (\n",
        "            f\"Introspection:\\n\"\n",
        "            f\"- The AI's introspection confidence is {introspection_quality:.3f}.\\n\"\n",
        "            f\"- Confidence is lower because the belief update had competing evidence, meaning multiple interpretations were possible.\"\n",
        "        )\n",
        "\n",
        "        return f\"{belief_update_text}\\n\\n{efe_explanation}\\n\\n{introspection_explanation}\"\n",
        "\n",
        "# Running the model\n",
        "state_size = 5\n",
        "action_size = 3\n",
        "\n",
        "ai_system = ExplainableGenerativeAI(state_size, action_size)\n",
        "\n",
        "# Example observation\n",
        "observation = np.random.randint(0, state_size)\n",
        "\n",
        "# Perform Bayesian state inference\n",
        "ai_system.infer_states(observation)\n",
        "\n",
        "# Infer the best policy\n",
        "ai_system.infer_policies()\n",
        "\n",
        "# Select an action\n",
        "action = ai_system.select_action()\n",
        "\n",
        "# Explain the decision\n",
        "explanation = ai_system.explain_decision()\n",
        "explanation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "3HKUYtg35bqS",
        "outputId": "c093b3f0-9cc5-49b6-8925-dbaa4c22cc3e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The AI originally believed it was in state 0, but observation led it to shift its belief towards state 4. The biggest belief shift occurred in state 4, changing by 0.193, driven by observation 4 with likelihood 0.892.\\n\\nAction Selection:\\n- The AI chose action 1 because it had the lowest Expected Free Energy (EFE): 5.111.\\n- This means action 1 is expected to provide the best trade-off between gaining useful information and achieving goals.\\n- Epistemic Value (expected uncertainty reduction): 1.239.\\n- Pragmatic Value (goal alignment): 3.872.\\n- The AI rejected action 2 because it had a lower epistemic value (1.327) and a less favorable goal alignment (5.660).\\n\\nIntrospection:\\n- The AI's introspection confidence is 0.200.\\n- Confidence is lower because the belief update had competing evidence, meaning multiple interpretations were possible.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9AFGKqfG8Bg_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}