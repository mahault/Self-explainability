{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0SJch5ORmhis8uf9v+ZoF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahault/Self-explainability/blob/main/explainability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJxWRPWrycWR",
        "outputId": "6d2f9146-a120-464a-9779-f2c073b8b16c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The AI chose its action primarily based on state 2, which had the highest influence score (0.049). The AI is confident in its self-explanation.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class ExplainableActiveInference:\n",
        "    \"\"\"\n",
        "    Implements an explainable AI agent using Active Inference principles.\n",
        "    This model replaces static influence scoring with a hierarchical generative model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, state_size):\n",
        "        # Initialize priors and uncertainties\n",
        "        self.state_size = state_size\n",
        "        self.prior_influence = np.zeros(state_size)  # Influence scores for each state\n",
        "        self.belief_uncertainty = np.ones(state_size)  # Higher means more uncertainty\n",
        "\n",
        "    def infer_influence(self, selected_action, qs, B):\n",
        "        \"\"\"\n",
        "        Infers influence of past states on action selection using Bayesian updates.\n",
        "        Returns:\n",
        "            influence_scores: Bayesian-updated influence estimates\n",
        "            uncertainty_scores: Self-tracked uncertainty of influence\n",
        "        \"\"\"\n",
        "        ACTIONABLE_ROW = 0\n",
        "        action_taken = selected_action[ACTIONABLE_ROW]\n",
        "        latest_beliefs = qs[ACTIONABLE_ROW]  # Belief distribution over states\n",
        "\n",
        "        influence_scores = np.zeros(self.state_size)\n",
        "        uncertainty_scores = np.zeros(self.state_size)\n",
        "\n",
        "        for state_idx in range(self.state_size):\n",
        "            belief = latest_beliefs[state_idx]  # Probability of being in this state\n",
        "\n",
        "            # Probabilistic transition to next state\n",
        "            if hasattr(B, '__getitem__'):\n",
        "                next_state_prob = B[ACTIONABLE_ROW][state_idx]\n",
        "            else:\n",
        "                next_state_prob = np.eye(self.state_size)[state_idx]  # Identity transition if B is missing\n",
        "\n",
        "            # Expected free energy contribution (Bayesian approach)\n",
        "            influence_score = belief * np.mean(next_state_prob)\n",
        "\n",
        "            # Bayesian Update of Prior Belief\n",
        "            self.prior_influence[state_idx] = (self.prior_influence[state_idx] + influence_score) / 2\n",
        "\n",
        "            # Update uncertainty: tracking variance of influence scores\n",
        "            uncertainty_scores[state_idx] = np.std([self.prior_influence[state_idx], influence_score])\n",
        "\n",
        "        self.belief_uncertainty = uncertainty_scores\n",
        "        return self.prior_influence, uncertainty_scores\n",
        "\n",
        "    def introspection_score(self):\n",
        "        \"\"\"\n",
        "        Measures how well the AI understands its own decision influences.\n",
        "        Returns:\n",
        "            A score between 0 and 1 (higher = more self-aware)\n",
        "        \"\"\"\n",
        "        mean_uncertainty = np.mean(self.belief_uncertainty)\n",
        "        return 1 / (1 + mean_uncertainty)  # Higher score means lower uncertainty (better explainability)\n",
        "\n",
        "    def explain_decision(self):\n",
        "        \"\"\"\n",
        "        Generates a human-readable explanation of why the AI made its decision.\n",
        "        Returns:\n",
        "            A textual explanation based on influence and introspection scores.\n",
        "        \"\"\"\n",
        "        max_influence_state = np.argmax(self.prior_influence)\n",
        "        introspection_quality = self.introspection_score()\n",
        "\n",
        "        explanation = (\n",
        "            f\"The AI chose its action primarily based on state {max_influence_state}, \"\n",
        "            f\"which had the highest influence score ({self.prior_influence[max_influence_state]:.3f}). \"\n",
        "        )\n",
        "\n",
        "        if introspection_quality > 0.8:\n",
        "            explanation += \"The AI is confident in its self-explanation.\"\n",
        "        elif introspection_quality > 0.5:\n",
        "            explanation += \"The AI has moderate confidence in its explanation.\"\n",
        "        else:\n",
        "            explanation += \"The AI has high uncertainty about why it made this decision.\"\n",
        "\n",
        "        return explanation\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    state_size = 10  # Assume 10 possible states\n",
        "    ai_system = ExplainableActiveInference(state_size)\n",
        "\n",
        "    # Example action (4 possible actions)\n",
        "    selected_action = np.array([0.1, 0.2, 0.5, 0.2])\n",
        "\n",
        "    # Example belief state distribution\n",
        "    qs = np.array([[0.05, 0.1, 0.3, 0.1, 0.15, 0.1, 0.05, 0.05, 0.05, 0.05]])\n",
        "\n",
        "    # Example transition model (B-matrix)\n",
        "    B = np.random.rand(1, state_size, state_size)  # Random state transitions\n",
        "\n",
        "    # Perform Bayesian influence inference\n",
        "    influence_scores, uncertainty_scores = ai_system.infer_influence(selected_action, qs, B)\n",
        "\n",
        "    # Print explanation\n",
        "    print(ai_system.explain_decision())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KZFYZBkxydt1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}