{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOavOeUl/9wry9rGw7bEjqq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahault/Self-explainability/blob/main/explainability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJxWRPWrycWR",
        "outputId": "6d2f9146-a120-464a-9779-f2c073b8b16c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The AI chose its action primarily based on state 2, which had the highest influence score (0.049). The AI is confident in its self-explanation.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class ExplainableActiveInference:\n",
        "    \"\"\"\n",
        "    Implements an explainable AI agent using Active Inference principles.\n",
        "    This model replaces static influence scoring with a hierarchical generative model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, state_size):\n",
        "        # Initialize priors and uncertainties\n",
        "        self.state_size = state_size\n",
        "        self.prior_influence = np.zeros(state_size)  # Influence scores for each state\n",
        "        self.belief_uncertainty = np.ones(state_size)  # Higher means more uncertainty\n",
        "\n",
        "    def infer_influence(self, selected_action, qs, B):\n",
        "        \"\"\"\n",
        "        Infers influence of past states on action selection using Bayesian updates.\n",
        "        Returns:\n",
        "            influence_scores: Bayesian-updated influence estimates\n",
        "            uncertainty_scores: Self-tracked uncertainty of influence\n",
        "        \"\"\"\n",
        "        ACTIONABLE_ROW = 0\n",
        "        action_taken = selected_action[ACTIONABLE_ROW]\n",
        "        latest_beliefs = qs[ACTIONABLE_ROW]  # Belief distribution over states\n",
        "\n",
        "        influence_scores = np.zeros(self.state_size)\n",
        "        uncertainty_scores = np.zeros(self.state_size)\n",
        "\n",
        "        for state_idx in range(self.state_size):\n",
        "            belief = latest_beliefs[state_idx]  # Probability of being in this state\n",
        "\n",
        "            # Probabilistic transition to next state\n",
        "            if hasattr(B, '__getitem__'):\n",
        "                next_state_prob = B[ACTIONABLE_ROW][state_idx]\n",
        "            else:\n",
        "                next_state_prob = np.eye(self.state_size)[state_idx]  # Identity transition if B is missing\n",
        "\n",
        "            # Expected free energy contribution (Bayesian approach)\n",
        "            influence_score = belief * np.mean(next_state_prob)\n",
        "\n",
        "            # Bayesian Update of Prior Belief\n",
        "            self.prior_influence[state_idx] = (self.prior_influence[state_idx] + influence_score) / 2\n",
        "\n",
        "            # Update uncertainty: tracking variance of influence scores\n",
        "            uncertainty_scores[state_idx] = np.std([self.prior_influence[state_idx], influence_score])\n",
        "\n",
        "        self.belief_uncertainty = uncertainty_scores\n",
        "        return self.prior_influence, uncertainty_scores\n",
        "\n",
        "    def introspection_score(self):\n",
        "        \"\"\"\n",
        "        Measures how well the AI understands its own decision influences.\n",
        "        Returns:\n",
        "            A score between 0 and 1 (higher = more self-aware)\n",
        "        \"\"\"\n",
        "        mean_uncertainty = np.mean(self.belief_uncertainty)\n",
        "        return 1 / (1 + mean_uncertainty)  # Higher score means lower uncertainty (better explainability)\n",
        "\n",
        "    def explain_decision(self):\n",
        "        \"\"\"\n",
        "        Generates a human-readable explanation of why the AI made its decision.\n",
        "        Returns:\n",
        "            A textual explanation based on influence and introspection scores.\n",
        "        \"\"\"\n",
        "        max_influence_state = np.argmax(self.prior_influence)\n",
        "        introspection_quality = self.introspection_score()\n",
        "\n",
        "        explanation = (\n",
        "            f\"The AI chose its action primarily based on state {max_influence_state}, \"\n",
        "            f\"which had the highest influence score ({self.prior_influence[max_influence_state]:.3f}). \"\n",
        "        )\n",
        "\n",
        "        if introspection_quality > 0.8:\n",
        "            explanation += \"The AI is confident in its self-explanation.\"\n",
        "        elif introspection_quality > 0.5:\n",
        "            explanation += \"The AI has moderate confidence in its explanation.\"\n",
        "        else:\n",
        "            explanation += \"The AI has high uncertainty about why it made this decision.\"\n",
        "\n",
        "        return explanation\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    state_size = 10  # Assume 10 possible states\n",
        "    ai_system = ExplainableActiveInference(state_size)\n",
        "\n",
        "    # Example action (4 possible actions)\n",
        "    selected_action = np.array([0.1, 0.2, 0.5, 0.2])\n",
        "\n",
        "    # Example belief state distribution\n",
        "    qs = np.array([[0.05, 0.1, 0.3, 0.1, 0.15, 0.1, 0.05, 0.05, 0.05, 0.05]])\n",
        "\n",
        "    # Example transition model (B-matrix)\n",
        "    B = np.random.rand(1, state_size, state_size)  # Random state transitions\n",
        "\n",
        "    # Perform Bayesian influence inference\n",
        "    influence_scores, uncertainty_scores = ai_system.infer_influence(selected_action, qs, B)\n",
        "\n",
        "    # Print explanation\n",
        "    print(ai_system.explain_decision())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class HierarchicalGenerativeModelAI:\n",
        "    \"\"\"\n",
        "    Implements a hierarchical generative model for explainable AI.\n",
        "    This model has three levels:\n",
        "    1. Overt action layer: standard Active Inference.\n",
        "    2. Covert policy selection layer: tracks influence of beliefs on actions.\n",
        "    3. Meta-introspection layer: evaluates self-awareness of decision-making.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # **Level 1: Beliefs about states**\n",
        "        self.qs = np.ones(state_size) / state_size  # Belief over states\n",
        "        self.B = np.random.rand(state_size, state_size)  # Transition matrix\n",
        "        self.A = np.random.rand(state_size, state_size)  # Likelihood matrix P(o|s)\n",
        "\n",
        "        # **Level 2: Policy selection**\n",
        "        self.qpi = np.ones(action_size) / action_size  # Belief over policies\n",
        "        self.G = np.random.rand(action_size)  # Expected Free Energy (EFE)\n",
        "\n",
        "        # **Level 3: Introspection layer**\n",
        "        self.U = np.ones(state_size) / state_size  # Belief about self-awareness\n",
        "        self.gamma = np.random.rand(state_size)  # Attentional weight for introspection\n",
        "\n",
        "    def infer_states(self, observation):\n",
        "        \"\"\"\n",
        "        Level 1: Bayesian state inference P(s_t | o_t).\n",
        "        \"\"\"\n",
        "        likelihood = self.A[:, observation]  # P(o_t | s_t)\n",
        "        self.qs = likelihood * self.qs  # Bayesian update\n",
        "        self.qs /= np.sum(self.qs)  # Normalize\n",
        "\n",
        "    def infer_policies(self):\n",
        "        \"\"\"\n",
        "        Level 2: Infer best policy using Expected Free Energy (EFE).\n",
        "        \"\"\"\n",
        "        expected_outcomes = self.B @ self.qs  # Predicted future states\n",
        "\n",
        "        # Ensure probabilities sum to 1 (avoid division by zero)\n",
        "        expected_outcomes = expected_outcomes / (np.sum(expected_outcomes) + 1e-6)\n",
        "\n",
        "        # Compute Expected Free Energy (EFE) per action (assume B maps to actions)\n",
        "        self.G = -np.sum(expected_outcomes * np.log(expected_outcomes + 1e-6))  # Still a scalar\n",
        "\n",
        "        # Ensure `G` is a vector over `action_size`\n",
        "        self.G = np.repeat(self.G, self.action_size)  # Expand into a vector\n",
        "\n",
        "        # Convert EFE to policy probabilities\n",
        "        self.qpi = np.exp(-self.G)  # Convert EFE scores to probabilities\n",
        "        self.qpi /= np.sum(self.qpi)  # Normalize\n",
        "\n",
        "        # Validate that qpi is properly normalized\n",
        "        if not np.isclose(np.sum(self.qpi), 1.0, atol=1e-6):\n",
        "            raise ValueError(f\"qpi normalization failed, sum={np.sum(self.qpi)}\")\n",
        "\n",
        "        # Ensure `qpi` has the correct shape\n",
        "        if self.qpi.shape[0] != self.action_size:\n",
        "            raise ValueError(f\"qpi must be a probability vector of size {self.action_size}, got shape {self.qpi.shape}\")\n",
        "\n",
        "\n",
        "    def infer_introspection(self):\n",
        "        \"\"\"\n",
        "        Level 3: Meta-awareness of decision-making confidence.\n",
        "        \"\"\"\n",
        "        self.U = 1 / (1 + np.std(self.qs))  # Self-awareness is inverse of uncertainty\n",
        "        self.gamma = np.exp(-self.U)  # Attentional modulation based on introspection\n",
        "\n",
        "    def select_action(self):\n",
        "        \"\"\"\n",
        "        Sample an action from the inferred policy.\n",
        "        \"\"\"\n",
        "        if len(self.qpi.shape) == 0 or len(self.qpi) != self.action_size:\n",
        "            raise ValueError(f\"qpi must be a probability vector of size {self.action_size}, got {self.qpi.shape}\")\n",
        "\n",
        "        action = np.random.choice(self.action_size, p=self.qpi)\n",
        "        return action\n",
        "\n",
        "\n",
        "    def explain_decision(self):\n",
        "        \"\"\"\n",
        "        Generates a hierarchical explanation of the AI's decision.\n",
        "        \"\"\"\n",
        "        chosen_state = np.argmax(self.qs)\n",
        "        introspection_quality = np.mean(self.U)\n",
        "\n",
        "        explanation = (\n",
        "            f\"Level 1: The AI believes it is in state {chosen_state} \"\n",
        "            f\"with probability {self.qs[chosen_state]:.3f}.\\n\"\n",
        "            f\"Level 2: The selected policy has an expected free energy of {self.G.min():.3f}.\\n\"\n",
        "            f\"Level 3: The AI's introspection confidence is {introspection_quality:.3f}.\"\n",
        "        )\n",
        "\n",
        "        if introspection_quality > 0.8:\n",
        "            explanation += \" The AI is highly confident in its explanation.\"\n",
        "        elif introspection_quality > 0.5:\n",
        "            explanation += \" The AI has moderate confidence in its explanation.\"\n",
        "        else:\n",
        "            explanation += \" The AI has high uncertainty about its reasoning.\"\n",
        "\n",
        "        return explanation\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    state_size = 5  # Assume 5 possible states\n",
        "    action_size = 3  # Assume 3 possible actions\n",
        "\n",
        "    ai_system = HierarchicalGenerativeModelAI(state_size, action_size)\n",
        "\n",
        "    # Example observation (randomly chosen)\n",
        "    observation = np.random.randint(0, state_size)\n",
        "\n",
        "    # Perform Bayesian state inference\n",
        "    ai_system.infer_states(observation)\n",
        "\n",
        "    # Infer the best policy\n",
        "    ai_system.infer_policies()\n",
        "\n",
        "    # Perform meta-introspection\n",
        "    ai_system.infer_introspection()\n",
        "\n",
        "    # Select an action\n",
        "    action = ai_system.select_action()\n",
        "\n",
        "    # Explain the decision\n",
        "    print(ai_system.explain_decision())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZFYZBkxydt1",
        "outputId": "cd0e3bf9-9c80-4014-a5bf-e72221c41eed"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Level 1: The AI believes it is in state 2 with probability 0.246.\n",
            "Level 2: The selected policy has an expected free energy of 1.542.\n",
            "Level 3: The AI's introspection confidence is 0.943. The AI is highly confident in its explanation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class ExplainableActiveInference:\n",
        "    \"\"\"\n",
        "    Implements an explainable AI agent using Active Inference principles.\n",
        "    Now explicitly tracks belief updates and action selection reasoning.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # **Level 1: Beliefs about states**\n",
        "        self.qs = np.ones(state_size) / state_size  # Belief over states\n",
        "        self.B = np.random.rand(state_size, state_size)  # Transition matrix\n",
        "        self.A = np.random.rand(state_size, state_size)  # Likelihood matrix P(o|s)\n",
        "\n",
        "        # **Level 2: Policy selection**\n",
        "        self.qpi = np.ones(action_size) / action_size  # Belief over policies\n",
        "        self.G = np.zeros(action_size)  # Expected Free Energy (EFE)\n",
        "\n",
        "        # **Level 3: Introspection layer**\n",
        "        self.U = np.ones(state_size) / state_size  # Belief about self-awareness\n",
        "        self.gamma = np.random.rand(state_size)  # Attentional weight for introspection\n",
        "\n",
        "        # Storage for explainability\n",
        "        self.belief_trace = []\n",
        "        self.action_trace = []\n",
        "\n",
        "    def infer_states(self, observation):\n",
        "        \"\"\"\n",
        "        Level 1: Bayesian state inference P(s_t | o_t).\n",
        "        Now tracks which observations contributed most to belief updates.\n",
        "        \"\"\"\n",
        "        likelihood = self.A[:, observation]  # P(o_t | s_t)\n",
        "        prior_belief = self.qs.copy()\n",
        "\n",
        "        # Bayesian update\n",
        "        self.qs = likelihood * self.qs\n",
        "        self.qs /= np.sum(self.qs)  # Normalize\n",
        "\n",
        "        # Identify which state changed the most\n",
        "        belief_changes = np.abs(self.qs - prior_belief)\n",
        "        most_updated_state = np.argmax(belief_changes)\n",
        "\n",
        "        # Store trace of belief update\n",
        "        self.belief_trace.append({\n",
        "            \"prior_belief\": prior_belief,\n",
        "            \"updated_belief\": self.qs,\n",
        "            \"most_updated_state\": most_updated_state,\n",
        "            \"likelihood_used\": likelihood\n",
        "        })\n",
        "\n",
        "    def infer_policies(self):\n",
        "        \"\"\"\n",
        "        Level 2: Infer best policy using Expected Free Energy (EFE).\n",
        "        Now explicitly tracks rejected alternative actions.\n",
        "        \"\"\"\n",
        "        expected_outcomes = self.B @ self.qs  # Predicted future states\n",
        "\n",
        "        # Compute Expected Free Energy (EFE) per action\n",
        "        for action in range(self.action_size):\n",
        "            # Simulate taking an action by considering expected transition\n",
        "            future_state = expected_outcomes[action]\n",
        "            self.G[action] = -np.sum(future_state * np.log(future_state + 1e-6))  # EFE\n",
        "\n",
        "        # Convert EFE to policy probabilities\n",
        "        self.qpi = np.exp(-self.G)\n",
        "        self.qpi /= np.sum(self.qpi)\n",
        "\n",
        "        # Store action reasoning trace\n",
        "        sorted_actions = np.argsort(self.G)  # Rank actions by EFE (lower is better)\n",
        "        self.action_trace.append({\n",
        "            \"EFE_values\": self.G,\n",
        "            \"best_action\": sorted_actions[0],\n",
        "            \"rejected_actions\": sorted_actions[1:].tolist()\n",
        "        })\n",
        "\n",
        "    def select_action(self):\n",
        "        \"\"\"\n",
        "        Selects an action based on the inferred policy.\n",
        "        \"\"\"\n",
        "        action = np.random.choice(self.action_size, p=self.qpi)\n",
        "        return action\n",
        "\n",
        "    def explain_decision(self):\n",
        "        \"\"\"\n",
        "        Generates a structured explanation including belief updates and action selection.\n",
        "        \"\"\"\n",
        "        chosen_state = np.argmax(self.qs)\n",
        "        introspection_quality = np.mean(self.U)\n",
        "\n",
        "        # Get belief update reasoning\n",
        "        belief_info = self.belief_trace[-1]\n",
        "        action_info = self.action_trace[-1]\n",
        "\n",
        "        explanation = (\n",
        "            f\"Level 1: The AI believes it is in state {chosen_state} \"\n",
        "            f\"with probability {self.qs[chosen_state]:.3f}, based on observation likelihoods {belief_info['likelihood_used']}.\\n\"\n",
        "            f\"Most updated state: {belief_info['most_updated_state']}.\\n\"\n",
        "            f\"Level 2: The selected policy has an expected free energy of {self.G.min():.3f}.\\n\"\n",
        "            f\"Action Selection: The AI chose action {action_info['best_action']} \"\n",
        "            f\"because it had the lowest EFE ({self.G[action_info['best_action']]:.3f}).\\n\"\n",
        "            f\"Rejected actions: {action_info['rejected_actions']} \"\n",
        "            f\"with higher EFEs: {[self.G[a] for a in action_info['rejected_actions']]}.\\n\"\n",
        "            f\"Level 3: The AI's introspection confidence is {introspection_quality:.3f}.\"\n",
        "        )\n",
        "\n",
        "        return explanation\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    state_size = 5\n",
        "    action_size = 3\n",
        "\n",
        "    ai_system = ExplainableActiveInference(state_size, action_size)\n",
        "\n",
        "    # Example observation\n",
        "    observation = np.random.randint(0, state_size)\n",
        "\n",
        "    # Perform Bayesian state inference\n",
        "    ai_system.infer_states(observation)\n",
        "\n",
        "    # Infer the best policy\n",
        "    ai_system.infer_policies()\n",
        "\n",
        "    # Select an action\n",
        "    action = ai_system.select_action()\n",
        "\n",
        "    # Explain the decision\n",
        "    print(ai_system.explain_decision())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYEWzRIk0jZ0",
        "outputId": "649cab58-44da-4603-fbdd-7e6c602fd76a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Level 1: The AI believes it is in state 2 with probability 0.656, based on observation likelihoods [0.10598477 0.28071784 0.90216317 0.0496934  0.03588424].\n",
            "Most updated state: 2.\n",
            "Level 2: The selected policy has an expected free energy of 0.306.\n",
            "Action Selection: The AI chose action 1 because it had the lowest EFE (0.306).\n",
            "Rejected actions: [0, 2] with higher EFEs: [0.358424395146506, 0.367870128218928].\n",
            "Level 3: The AI's introspection confidence is 0.200.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "plg7RD_x4ep-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}